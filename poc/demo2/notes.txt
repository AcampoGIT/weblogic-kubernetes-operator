-----------------------------------------------------------------------------------------
scenario
-----------------------------------------------------------------------------------------

  one kubernetes cluster with two operators,
  each with its own domains namespace and one domain

  separate the lifecycles by installing 3 charts:
    - setup shared kubernetes cluster artifacts
      (roles, kibana, elastic search)
    - operator1
    - operator2

  operator customizations
    separate kubenetes cluster setup from operator creation
    elk enabled
    enable external REST
    custom namespace names
    custom operator image

  domain home in shared pv

  domain customizations
    custom namespace names
    custom pv/pvc (for the domain home)
    extra env vars for the servers, different ones for the admin server

-----------------------------------------------------------------------------------------
setup
-----------------------------------------------------------------------------------------

  doublesync src122130 and devenv.sh
    (since we're going to use WLST to create the domain)

  mkdir -m 777 -p /scratch/k8s-dir
    (i.e. the parent directory for persistent volumes)

  make sure to get rid your old setup:
  (I had problems getting stuff to run until I got rid of this stuff)
    operator and domain namespaces
    persistent volumes
    cluster roles & cluster role bindings
    maybe /scratch/k8s_dir

  install helm & tiller
    See https://github.com/kubernetes/helm/blob/master/docs/install.md

-----------------------------------------------------------------------------------------
how to run demo2
-----------------------------------------------------------------------------------------

  cd demo2

  mkdir generated # the scripts need to be improved to do this!

  helm install ../kit/charts/operator --name demo2-kubernetes-cluster --values kubernetes-cluster-values.yaml --wait

  cp operator1-values.yaml.orig operator1-values.yaml
  ../kit/scripts/generate-internal-operator-certificate.sh demo2-o1-ns >> operator1-values.yaml
  ../kit/scripts/generate-external-operator-certificate.sh DNS:localhost >> operator1-values.yaml
  helm install ../kit/charts/operator --name demo2-operator1 --values operator1-values.yaml --wait
  kubectl -n demo2-d1-ns create secret generic demo2-domain1-uid-domain-creds --from-literal=username=weblogic --from-literal=password=welcome1
  mkdir -p /scratch/k8s-dir/demo2-domain1-uid/domain-logs
  #from Tom Barnes: if running on a hosted linux box (macs don't need this):
  #  /usr/local/packages/aime/ias/run_as_root "find /scratch/k8s-dir/ -name '*' | xargs chown 1000"
  #  /usr/local/packages/aime/ias/run_as_root "find /scratch/k8s-dir/ -name '*' | xargs chgrp 1000"
  create-domain-home.sh 1
  helm install ../kit/charts/domain --name demo2-domain1 --values domain1-values.yaml --wait

  cp operator2-values.yaml.orig operator2-values.yaml
  ../kit/scripts/generate-internal-operator-certificate.sh demo2-o2-ns >> operator2-values.yaml
  ../kit/scripts/generate-external-operator-certificate.sh DNS:localhost >> operator2-values.yaml
  helm install ../kit/charts/operator --name demo2-operator2 --values operator2-values.yaml --wait
  kubectl -n demo2-d2-ns create secret generic demo2-domain2-uid-domain-creds --from-literal=username=weblogic --from-literal=password=welcome1
  mkdir -p /scratch/k8s-dir/demo2-domain2-uid/domain-logs
  #from Tom Barnes: if running on a hosted linux box (macs don't need this):
  #  /usr/local/packages/aime/ias/run_as_root "find /scratch/k8s-dir/ -name '*' | xargs chown 1000"
  #  /usr/local/packages/aime/ias/run_as_root "find /scratch/k8s-dir/ -name '*' | xargs chgrp 1000"
  create-domain-home.sh 2
  helm install ../kit/charts/domain --name demo2-domain2 --values domain2-values.yaml --wait

  # TBD - something about creating the config map containing the domain-specific sit config customizations
  # and passing the name of that config map into the sit config stuff below ...

  # domain-crd (roughly)
  #   domainUID: demo2-domain1-uid
  #   clusterDefaults:
  #     replicas: 1
  #   adminServer:
  #     template: debug

  # operator runtime tasks

  ../kit/runtime/simulate-operator-introspect-domain.sh default demo2-d1-ns demo2-domain1-uid
  ../kit/runtime/simulate-operator-introspect-domain.sh debug demo2-d1-ns demo2-domain1-uid
  ../kit/runtime/simulate-operator-start-admin-server.sh debug demo2-o1-ns demo2-d1-ns demo2-domain1-uid demo2-domain1 as 7100 RUNNING
  ../kit/runtime/simulate-operator-start-managed-server.sh default demo2-o1-ns demo2-d1-ns demo2-domain1-uid demo2-domain1 as 7100 ms1 8100 RUNNING
  ../kit/runtime/simulate-operator-stop-managed-server.sh demo2-d1-ns demo2-domain1-uid ms1
  ../kit/runtime/simulate-operator-stop-admin-server.sh demo2-d1-ns demo2-domain1-uid as
  ../kit/runtime/simulate-operator-remove-server-resources.sh debug demo2-domain1-uid
  ../kit/runtime/simulate-operator-remove-server-resources.sh default demo2-domain1-uid

  helm delete --purge demo2-domain2
  kubectl delete secret -n demo2-d2-ns demo2-domain2-uid-domain-creds
  #from Tom Barnes: if running on a hosted linux box (macs don't need this):
  #  /usr/local/packages/aime/ias/run_as_root "find /scratch/k8s-dir/ -name '*' | xargs chmod 777"
  rm -rf /scratch/k8s-dir/demo2-domain2-uid
  helm delete --purge demo2-operator2

  helm delete --purge demo2-domain1
  kubectl delete secret -n demo2-d1-ns demo2-domain1-uid-domain-creds
  #from Tom Barnes: if running on a hosted linux box (macs don't need this):
  #  /usr/local/packages/aime/ias/run_as_root "find /scratch/k8s-dir/ -name '*' | xargs chmod 777"
  rm -rf /scratch/k8s-dir/demo2-domain1-uid
  helm delete --purge demo2-operator1

  helm delete --purge demo2-kubernetes-cluster

-----------------------------------------------------------------------------------------
poc source files
-----------------------------------------------------------------------------------------

  demo2/
    uses offline wlst to create a domain with a configured cluster
    uses a domain persistent volume
    uses helm to configure everything

    kubernetes-cluster-values.yaml
      - helm configuration settings for setting up the kubernets cluster kubernetes resources

    operator-values.yaml
      - helm configuration settings for creating the operator kubernetes resources

    domains-ns-values.yaml
      - helm configuration settings for creating the domains namespace kubernetes resources

    domain-values.yaml
      - helm configuration settings for creating the domain kubernetes resources

    create-domain-home.sh
      - shell script that uses offline wlst to configure a domain home with a configured cluster

-----------------------------------------------------------------------------------------
poc generated files
-----------------------------------------------------------------------------------------

    demo2/generated/

        domain-home
          - contains the generated domain, before its pathnames have been patched
          - this means you can 'cd' there and run startWeblogicServer.sh
          - it also means that it cannot be directly used by a pod since the
            pathnames in the generated files are based on the shell that ran
            create-domain-home.sh, instead of the ones that are needed in a pod

    /scratch/k8s-dir/demo2-domain1-uid
    /scratch/k8s-dir/demo2-domain2-uid
      contains the persistent volumes for demo2's domains

      domain-logs/
         - contains the domain, node manager and server logs

      domain-home/
        - contains the domain home that the pods use
        - the pathnames in the files have been patched so that the pods can use them
          (e.g. domain home, java home and mw home have been changed to the values
          that should be used inside the pod)

-----------------------------------------------------------------------------------------
todo
-----------------------------------------------------------------------------------------

pass thru customizations needed to create the sit cfg from user thru to
sit cfg generator (even though it won't pay attention to them yet)

demo2-domain-crd (roughly)

  domainUID: demo2-domain-uid

  domainIntrospector: demo2-domain-uid-default-domain-introspector-cm

  serverDefaults:
    domainCustomizations:  demo2-domain-uid-domain-customizations-cm
    sitConfigGenerator:    demo2-domain-uid-default-sitcfg-generator-cm 
    serverPodTemplate:     demo2-domain-uid-default-managed-server-pod-template-cm
    serverServiceTemplate: demo2-domain-uid-default-managed-server-service-template-cm

  adminServer:
    serverPodTemplate:       demo2-domain-uid-debug-admin-server-pod-template-cm
    serverServiceTemplate:   demo2-domain-uid-default-admin-server-service-template-cm
    serverT3ServiceTemplate: demo2-domain-uid-default-admin-server-t3-service-template-cm

possible formats for having the customer specify domain config property overridesA:

properties:

domain.server.name.as.list-address:abcd
domain.server.name.as.list-port:7001

yaml:

domain:
  server:
    name:as
    listen-address: abcd
    listen-port: 7001

make prometheus annotations optional?

try to make a template for most of the pod stuff?






overall

cluster side operator setup
  helm chart
  vars:
    elkEnabled

operator setup
  helm chart
  vars:
    externally generated certs
    most of the stuff from the old operator inputs file

domains namespace setup
  helm chart
  don't need much - mostly a namespace name

domain setup
  lots here

domain introspection
  v.s. configuring admin server name & port
  done in offline wlst in a pod w/ access to the domain home
  also does domain validation so we can tell if the operator can handle the domain

sitcfg generation
  done in offline wlst in a pod w/ access to the domain home
  need a way for the customer to add customizations
  results in a config map (with a sit cfg file) that gets mounted into the server pod

domain.values
  non-config-xml info about the domain
  templates for pods, services, ...

domain custom resource
  lifecycle rules for servers & clusters
  pointers to the templates for creating k8s artifacts


need specs
  architecture
  functional - i.e. what a customer sees

need to implement

need wlst / jython guru
  best practices
  how do we test it?

need a helm guru
  best practices
    docs
    values validation
  slots for anything we want tweakable
    e.g. add volumes, tweak timing / retry params, add labels, ...
    or should we encourage customers to copy our helm charts & tweak? (probably a bad idea)
  how do we test them since they have a lot of conditional code?
    --dry-run ?
    require k8s & look at artifacts?

naming conventions
  so that templates, ... don't collide with servers

need a backwards compat and rollout strategy

how about
  customers must create new domains using our new mechanims
    we'll need to give at least doc guidelines
  all new operator/domain config - won't work w/ old configs, old input files
  all new operator runtime - won't work w/ old domain

phase1
  build new infra for configuring artifacts, helm based, independent of old create scripts, artifacts, inputs files
  simulate operator runtime w/ scripts

phase2
  build new operator runtime (i.e. a different image) that only handles new artifacts

phase3
  cut over from old operator to new operator

i.e. I don't think we can get there incrementally easily

---------------


helm links
  https://github.com/kubernetes/helm/blob/master/docs
  https://github.com/kubernetes/helm/tree/master/docs/examples
  https://godoc.org/text/template
  https://godoc.org/github.com/Masterminds/sprig
  https://github.com/sapcc/openstack-helm/tree/master
  https://github.com/kubernetes/helm/blob/master/docs/charts_tips_and_tricks.md
  https://docs.helm.sh/chart_template_guide/#../charts
  https://docs.helm.sh/chart_template_guide/#the-chart-template-developer-s-guide
  https://github.com/Masterminds/sprig/blob/master/docs/dicts.md
  https://github.com/kubernetes/charts/blob/master/stable/traefik
  https://github.com/kubernetes/helm/blob/master/docs/examples/nginx

turn on debugging for just the admin server
===========================================

domain-crd.yaml
---------------
domainUID: demo2-domain-uid
adminServer:
  podTemplate: debug

domain-values.yaml
------------------
minimum plus:

customAdminServerPodTemplates:
  debug:
    extraEnvVars:
    - debugEnabled: true



automatically switch the domain to a new image name
===================================================

domain-crd.yaml
---------------
domainUID: demo2-domain-uid

domain-values.yaml
------------------
minimum plus:
weblogicImage: v2image
(i.e. just change the image name - the operator will notice the stale pods
and gradually replace them)




manually switch the domain to a new image name
(i.e. customer controlled rollout)
===============================================

domain-crd.yaml
---------------
domainUID: demo2-domain-uid
adminServer:
  podTemplate: v2
clusters:
  cluster1:
    servers:
      server2:
        podTemplate: v2

domain-values.yaml
------------------
minimum plus:

weblogicImage: v1image

customPodTemplates:
  v2:
    weblogicImage: v2image

customAdminServerPodTemplates:
  v2:
    podTemplate: v2
    extraEnvVars:
    - debugEnabled: true

customManagedServerPodTemplates:
  v2:
    podTemplate: v2
    extraEnvVars:
    - debugEnabled: true




1) design the k8s artifacts

2) design the k8s templates

3) design the domain crd

4) use helm to create the k8s artifacts (and perhaps the templates)

5) modify the operator to use the new k8s artifacts, templates and domain crd
   (including finishing the lifecycle work we started earlier)


can we phase it in feature by feature?  what lower level infra cleanup do we want to do
to make it easier?

most important - domain home in image
  implies sit cfg right from the get go?
  implies domain validation right from the get go?

figure out which parts of the poc are needed by each feature

the service stuff needs a major revamp
  probably should introspect the domain to find out all the ports (listen port + naps)
  then, for each, if the customer specifies a nodePort, create a NodePort service for it,
  otherwise, create a ClusterIP service for it.

  today, the poc is always exposting the listen port and t3 nap under fixed numbers,
  thus demo2 and demo3 collide (prevents demo3's managed servers from coming up since
  they can't get to demo3's admin server since demo2 is already took its node port)
