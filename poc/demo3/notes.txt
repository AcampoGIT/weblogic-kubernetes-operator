-----------------------------------------------------------------------------------------
scenario
-----------------------------------------------------------------------------------------

  one operator, one domains namespace, one domain

  domain home in image

  operator customizations
    enable elk
    custom namespace names
    custom operator image

  domain customizations
    custom namespace names

-----------------------------------------------------------------------------------------
setup
-----------------------------------------------------------------------------------------

  doublesync src122130 and devenv.sh
    (since we're going to use WLST to create the domain)

  mkdir -m 777 -p /scratch/k8s-dir
    (i.e. the parent directory for persistent volumes)

  make sure to get rid your old setup:
  (I had problems getting stuff to run until I got rid of this stuff)
    operator and domain namespaces
    persistent volumes
    cluster roles & cluster role bindings
    maybe /scratch/k8s_dir

  install helm & tiller
    See https://github.com/kubernetes/helm/blob/master/docs/install.md

-----------------------------------------------------------------------------------------
how to run demo3
-----------------------------------------------------------------------------------------

  cd demo3

  # generate certs for the operator REST api
  # copy the values file for creating the operator and customize it (including certs)

  create-domain-in-image.sh

  helm install ../kit/charts/operator --name demo3-operator --values operator-values.yaml --wait

  kubectl -n demo3-d-ns create secret generic demo3-domain-uid-domain-creds --from-literal=username=weblogic --from-literal=password=welcome1

  mkdir -p /scratch/k8s-dir/demo3-domain-uid/domain-logs

  #from Tom Barnes: if running on a hosted linux box (macs don't need this):
  #  /usr/local/packages/aime/ias/run_as_root "find /scratch/k8s-dir/ -name '*' | xargs chown 1000"
  #  /usr/local/packages/aime/ias/run_as_root "find /scratch/k8s-dir/ -name '*' | xargs chgrp 1000"

  # copy the values file for creating a domain and customize it for this domain
  # TBD - something about specifying extra domain customizations beyond what the
  # operator knows it needs to do

  helm install ../kit/charts/domain --name demo3-domain --values domain-values.yaml --wait

  # domain crd (roughly)
  #   domainUID: demo3-domain-uid
  #   clusterDefaults:
  #     replicas: 1

  ../kit/runtime/simulate-operator-introspect-domain.sh demo3-d-ns demo3-domain-uid default
  ../kit/runtime/simulate-operator-generate-sitcfg.sh demo3-d-ns demo3-domain-uid default
  ../kit/runtime/simulate-operator-start-admin-server.sh demo3-d-ns demo3-domain-uid default default default demo3-domain as 7300
  ../kit/runtime/simulate-operator-start-managed-server.sh demo3-d-ns demo3-domain-uid default default default demo3-domain as 7300 ms1 8300
  ../kit/runtime/simulate-operator-stop-managed-server.sh demo3-d-ns demo3-domain-uid default default ms1
  ../kit/runtime/simulate-operator-stop-admin-server.sh demo3-d-ns demo3-domain-uid default default as
  ../kit/runtime/simulate-operator-remove-sitcfg.sh demo3-d-ns demo3-domain-uid default

  helm delete --purge demo3-domain

  kubectl delete secret -n demo-d-ns demo3-domain-uid-domain-creds
  #from Tom Barnes: if running on a hosted linux box (macs don't need this):
  #  /usr/local/packages/aime/ias/run_as_root "find /scratch/k8s-dir/ -name '*' | xargs chmod 777"
  rm -rf /scratch/k8s-dir/demo3-domain-uid
  docker rmi wls-12213-domain-in-image:latest

  helm delete --purge demo3-operator

-----------------------------------------------------------------------------------------
poc source files
-----------------------------------------------------------------------------------------

  demo3/
    creates a docker image with a domain home that has a configured (v.s. dynamic) cluster
    uses helm to configure everything

    operator-values.yaml
      - helm configuration settings for creating the operator kubernetes resources

    domain-values.yaml
      - helm configuration settings for creating the domain kubernetes resources

    create-domain-home.sh
      - shell script that uses offline wlst to create the domain home with a configured
        cluster and to create a docker image with that domain home

-----------------------------------------------------------------------------------------
poc generated files
-----------------------------------------------------------------------------------------

    /scratch/k8s-dir/demo3-domain-uid
      directory that parents any persistent volumes this demo needs

      domain-logs/
         - contains the domain, node manager and server logs

-----------------------------------------------------------------------------------------
todo
-----------------------------------------------------------------------------------------

pass thru customizations needed to create the sit cfg from user thru to
sit cfg generator (even though it won't pay attention to them yet)

demo3-domain-crd (roughly)

  domainUID: demo3-domain-uid

  domainIntrospector: demo3-domain-uid-default-domain-introspector-cm

  serverDefaults:
    domainCustomizations:  demo3-domain-uid-domain-customizations-cm
    sitConfigGenerator:    demo3-domain-uid-default-sitcfg-generator-cm
    serverPodTemplate:     demo3-domain-uid-default-managed-server-pod-template-cm
    serverServiceTemplate: demo3-domain-uid-default-managed-server-service-template-cm

  adminServer:
    serverPodTemplate:       demo3-domain-uid-debug-admin-server-pod-template-cm
    serverServiceTemplate:   demo3-domain-uid-default-admin-server-service-template-cm
    serverT3ServiceTemplate: demo3-domain-uid-default-admin-server-t3-service-template-cm

possible formats for having the customer specify domain config property overridesA:

properties:

domain.server.name.as.list-address:abcd
domain.server.name.as.list-port:7001

yaml:

domain:
  server:
    name:as
    listen-address: abcd
    listen-port: 7001

make prometheus annotations optional?

try to make a template for most of the pod stuff?






overall

cluster side operator setup
  helm chart
  vars:
    elkEnabled

operator setup
  helm chart
  vars:
    externally generated certs
    most of the stuff from the old operator inputs file

domains namespace setup
  helm chart
  don't need much - mostly a namespace name

domain setup
  lots here

domain introspection
  v.s. configuring admin server name & port
  done in offline wlst in a pod w/ access to the domain home
  also does domain validation so we can tell if the operator can handle the domain

sitcfg generation
  done in offline wlst in a pod w/ access to the domain home
  need a way for the customer to add customizations
  results in a config map (with a sit cfg file) that gets mounted into the server pod

domain.values
  non-config-xml info about the domain
  templates for pods, services, ...

domain custom resource
  lifecycle rules for servers & clusters
  pointers to the templates for creating k8s artifacts


need specs
  architecture
  functional - i.e. what a customer sees

need to implement

need wlst / jython guru
  best practices
  how do we test it?

need a helm guru
  best practices
    docs
    values validation
  slots for anything we want tweakable
    e.g. add volumes, tweak timing / retry params, add labels, ...
    or should we encourage customers to copy our helm charts & tweak? (probably a bad idea)
  how do we test them since they have a lot of conditional code?
    --dry-run ?
    require k8s & look at artifacts?

naming conventions
  so that templates, ... don't collide with servers

need a backwards compat and rollout strategy

how about
  customers must create new domains using our new mechanims
    we'll need to give at least doc guidelines
  all new operator/domain config - won't work w/ old configs, old input files
  all new operator runtime - won't work w/ old domain

phase1
  build new infra for configuring artifacts, helm based, independent of old create scripts, artifacts, inputs files
  simulate operator runtime w/ scripts

phase2
  build new operator runtime (i.e. a different image) that only handles new artifacts

phase3
  cut over from old operator to new operator

i.e. I don't think we can get there incrementally easily

---------------

slides

major problems we're trying to address

  more complex domains (v.s. our toy domains)

  domain home inside image

  lifecycle (i.e. user controlled order of restarts)

  give the user a lot more control over the k8s artifacts we create
    - extra volume mounts
    - change tuning parameters
    - general issue - they keep asking to customize stuff we haven't thought of before

  give the user a lot more control over how/which wls ports and network access points are exposed

  let user override wls properties (e.g. bind the domain - creds, addresses, ...)

  domain introspection

  validate that a domain can be supported by the operator

  formalize levels of lifecycle so we can cleanly setup and teardown

  move the k8s artifact templates outside of the operator java code and into mounts
  (since it will be easier to update files than to update the operator runtime)

  add helm support

topology

  k8s cluster -> n operators -> n domain namespaces -> no domains

  implies need for 4 levels of lifecycle

slide for each lifecycle level

slide showing domain crd

slide showing operator runtime

slide showing domain introspection

slide showing sit config generation


big questions

what should the granularity of introspector be?
  i.e. to add more servers, the customer spins a new image with a new domain home,
  but we want to gradually roll out the change - how do we do this? i.e. what do we
  do when different servers have a different view of the domain config?

similar question for sit config
  since it modifies config.xml at runtime, is OK for different servers to have different
  sit configs?  and what about shared domain home on pv?
  i.e. as you roll out a new image, you might need:
    different values for the same settings on the new servers
      e.g. used to point to test db, now want to point to production db?
    what if the set of apps is different? ...
